{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Hyperparameter Tuning using HyperDrive\n",
        "\n",
        "TODO: Import Dependencies. In the cell below, import all the dependencies that you will need to complete the project."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.core import Workspace, Experiment, Environment, Datastore, Dataset\n",
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "# Setting up the workspace\n",
        "# From a config.json file\n",
        "ws = Workspace.from_config()\n",
        "\n",
        "# From a known workspace\n",
        "# workspace_name = os.environ.get('WORKSPACE_NAME', 'udacity-projects')\n",
        "# ws = Workspace.get(name=workspace_name)\n",
        "\n",
        "print('Workspace name: ' + ws.name, \n",
        "      'Azure region: ' + ws.location, \n",
        "      'Subscription id: ' + ws.subscription_id, \n",
        "      'Resource group: ' + ws.resource_group, sep = '\\n')\n",
        "\n",
        "# Setup the experiment\n",
        "experiment_name = os.environ.get('EXPERIMENT_NAME', 'az-capstone-hd')\n",
        "exp = Experiment(workspace=ws, name=experiment_name)\n",
        "\n",
        "# Setup the environment\n",
        "# From a Conda specification file\n",
        "env = Environment.from_conda_specification(name = \"az-capstone\", file_path = \"envs/env.yml\")\n",
        "\n",
        "# From a pip requirements file\n",
        "# env = Environment.from_pip_requirements(name = \"az-ml\", file_path = \"path-to-pip-requirements-file\")\n",
        "\n",
        "# Registering and building the environment\n",
        "env = env.register(workspace=ws)\n",
        "env_build = env.build(workspace=ws)\n",
        "\n",
        "# Enable logs\n",
        "run = exp.start_logging()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1644782228761
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creating relevant resources..."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.core.compute import ComputeTarget, AmlCompute\n",
        "\n",
        "# Setup the compute cluster\n",
        "# Use vm_size = \"Standard_D2_V2\" in your provisioning configuration.\n",
        "# max_nodes should be no greater than 4.\n",
        "compute_name = os.environ.get('CLUSTER_NAME', 'hd-cluster')\n",
        "compute_min_nodes = os.environ.get('CLUSTER_MIN_NODES', 0)\n",
        "compute_max_nodes = os.environ.get('CLUSTER_MAX_NODES', 4)\n",
        "vm_size = os.environ.get('CLUSTER_SKU', 'STANDARD_D2_V2')\n",
        "\n",
        "# Verify if the compute cluster exists\n",
        "if compute_name in ws.compute_targets:\n",
        "    compute_target = ws.compute_targets[compute_name]\n",
        "    if compute_target and type(compute_target) is AmlCompute:\n",
        "        print('found compute target. just use it. ' + compute_name)\n",
        "else:\n",
        "    print('creating a new compute target...')\n",
        "    provisioning_config = AmlCompute.provisioning_configuration(\n",
        "        vm_size=vm_size,\n",
        "        min_nodes=compute_min_nodes,\n",
        "        max_nodes=compute_max_nodes)\n",
        "\n",
        "    # create the cluster\n",
        "    compute_target = ComputeTarget.create(ws, compute_name, provisioning_config)\n",
        "\n",
        "    # poll for a minimum number of nodes and for a specific timeout.\n",
        "    # if no min node count is provided it will use the scale settings for the cluster\n",
        "    compute_target.wait_for_completion(show_output=True, min_node_count=None, timeout_in_minutes=30)\n",
        "\n",
        "     # For a more detailed view of current AmlCompute status, use get_status()\n",
        "    print(compute_target.get_status().serialize())"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1644782229048
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset\n",
        "\n",
        "TODO: Get data. In the cell below, write code to access the data you will be using in this project. Remember that the dataset needs to be external."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('data/ndf.csv')\n",
        "print('dataset shape: ', df.shape)\n",
        "print('columns:\\n', df.columns)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1644782229240
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "drop_col_list = ['y_close', 'y_c', 'y_close_shift', 'y_returns_shift']\n",
        "df.drop(columns=drop_col_list, inplace=True)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%%\n"
        },
        "gather": {
          "logged": 1644782229376
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Register the dataset\n",
        "datastore = ws.get_default_datastore()\n",
        "dataset = Dataset.Tabular.register_pandas_dataframe(df, datastore, \"hd_dataset\", show_progress=True)\n",
        "df = dataset.to_pandas_dataframe()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%%\n"
        },
        "gather": {
          "logged": 1644782234929
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hyperdrive Configuration\n",
        "\n",
        "TODO: Explain the model you are using and the reason for chosing the different hyperparameters, termination policy and config settings."
      ],
      "metadata": {
        "collapsed": true,
        "gather": {
          "logged": 1598531923519
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.widgets import RunDetails\n",
        "# from azureml.train.sklearn import SKLearn\n",
        "from azureml.train.hyperdrive.run import PrimaryMetricGoal\n",
        "from azureml.train.hyperdrive.policy import BanditPolicy\n",
        "from azureml.train.hyperdrive.sampling import RandomParameterSampling\n",
        "from azureml.train.hyperdrive.runconfig import HyperDriveConfig\n",
        "from azureml.train.hyperdrive.parameter_expressions import uniform, choice\n",
        "from azureml.core import ScriptRunConfig\n",
        "\n",
        "# Setup hyperparameter tuning\n",
        "\n",
        "# Specify parameter sampler\n",
        "ps = RandomParameterSampling(\n",
        "    {\n",
        "        'C': choice([x*0.001 for x in range(1,1000)]),\n",
        "        'max_iter': choice(range(100, 500))\n",
        "    }\n",
        ")\n",
        "\n",
        "# Specify a termination Policy\n",
        "policy = BanditPolicy(slack_factor=0.1)\n",
        "\n",
        "# Get the previously registered environment\n",
        "# env = Environment.get(workspace=ws, name=\"az-ml\")\n",
        "\n",
        "# Create an estimator for use with train.py and pass in the environment\n",
        "est = ScriptRunConfig(\n",
        "    source_directory=\"./scripts\",\n",
        "    script=\"train.py\",\n",
        "    compute_target=compute_target,\n",
        "    environment=env)\n",
        "\n",
        "# Create a HyperDriveConfig using the estimator, hyperparameter sampler, and policy.\n",
        "hyperdrive_run_config = HyperDriveConfig(\n",
        "    run_config=est,\n",
        "    hyperparameter_sampling=ps,\n",
        "    policy=policy,\n",
        "    primary_metric_name=\"accuracy\",\n",
        "    primary_metric_goal=PrimaryMetricGoal.MAXIMIZE,\n",
        "    max_total_runs=100,\n",
        "    max_concurrent_runs=4)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1644782235072
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Submit hyperdrive run to the experiment and show run details with the widget.\n",
        "hyperdrive_run = exp.submit(hyperdrive_run_config)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1644782237845
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run Details\n",
        "\n",
        "OPTIONAL: Write about the different models trained and their performance. Why do you think some models did better than others?\n",
        "\n",
        "TODO: In the cell below, use the `RunDetails` widget to show the different experiments."
      ],
      "metadata": {
        "collapsed": true,
        "gather": {
          "logged": 1598544898497
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "RunDetails(hyperdrive_run).show()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1644782238169
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Best Model\n",
        "\n",
        "TODO: In the cell below, get the best model from the hyperdrive experiments and display all the properties of the model."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "\n",
        "# Get best run and save the model from that run.\n",
        "best_run = hyperdrive_run.get_best_run_by_primary_metric()\n",
        "best_run_metrics = best_run.get_metrics()\n",
        "parameter_values = best_run.get_details()['runDefinition']['arguments']\n",
        "print(best_run_metrics)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1598546650307
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the best model\n",
        "print('Best Run Id: ', best_run.id)\n",
        "\n",
        "for i in best_run_metrics:\n",
        "    print(i, best_run_metrics[i])\n",
        "\n",
        "model = best_run.register_model(model_name='capstone-hd', model_path='./outputs/capstone-hd.joblib')\n",
        "model.download(target_dir=\"models\", exist_ok=True)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1598546657829
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Deployment\n",
        "\n",
        "Remember you have to deploy only one of the two models you trained but you still need to register both the models.\n",
        "Perform the steps in the rest of this notebook only if you wish to deploy this model.\n",
        "\n",
        "TODO: In the cell below, register the model, create an inference config and deploy the model as a web service."
      ],
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "\n",
        "# Get best run and save the model from that run.\n",
        "best_run = hyperdrive_run.get_best_run_by_primary_metric()\n",
        "best_run_metrics = best_run.get_metrics()\n",
        "parameter_values = best_run.get_details()['runDefinition']['arguments']\n",
        "print(best_run_metrics)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "print('Best Run Id: ', best_run.id)\n",
        "\n",
        "for i in best_run_metrics:\n",
        "    print(i, best_run_metrics[i])\n",
        "\n",
        "model = best_run.register_model(model_name='capstone-hd', model_path='./models/capstone-hd.joblib')\n",
        "model.download(target_dir=\"models\", exist_ok=True)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%%\n"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "TODO: In the cell below, send a request to the web service you deployed to test it."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "TODO: In the cell below, print the logs of the web service and delete the service"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "compute_target.delete()\n",
        "print('Compute cluster deleted!')"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Submission Checklist**\n",
        "- I have registered the model.\n",
        "- I have deployed the model with the best accuracy as a webservice.\n",
        "- I have tested the webservice by sending a request to the model endpoint.\n",
        "- I have deleted the webservice and shutdown all the computes that I have used.\n",
        "- I have taken a screenshot showing the model endpoint as active.\n",
        "- The project includes a file containing the environment details.\n",
        "\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {}
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "python3"
    },
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.9",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}