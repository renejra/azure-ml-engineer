# Optimizing an ML Pipeline in Azure

## Overview
This project is part of the Udacity Azure ML Nanodegree.
In this project, we build and optimize an Azure ML pipeline using the Python SDK and a provided Scikit-learn model.
This model is then compared to an Azure AutoML run.

## Summary
The dataset contains anonymous data about marketing campaigns of a bank, mainly focusing on demographics. 
The labels are composed of two values ('yes' or 'no') on the column named 'y'. 
The best performing model was obtained using AutoML, with an accuracy close to 91.7%, achieving a slight improvement of 0.2%
when compared to a Logistic Regressor with tuned hyperparameters.

## Scikit-learn Pipeline
We used a Logistic Regression model from Scikit-learn for performing classification, which we tuned using Hyperdrive.
The parameters we used for optimization where inverse regularization strength ('C') and
the maximum number of iterations ('max-iter'), aiming to get the best accuracy possible. 

For skewing and optimizing the different values of these parameters, we used random parameter sampler, which would help us find the
best combination of choices for both parameters. The usage of randomness reduces the need for extensive computation 
of performing grid search, while preserving good optimization capabilities. For the inverse regularization strenght, I used
a random choice of 1000 values between 0.001 and 1, while on the other hand, for the maximum iterations, I used a random value between
100 and 500 iterations.

```python
ps = RandomParameterSampling(
    {
        'C': choice([x*0.001 for x in range(1,1000)]),
        'max_iter': choice(range(100, 500))
    }
)
```
For early stopping, we chose Bandit Termination Policy, which would stop the experiments once further ones can't improve the
primary metric above the slack-factor (we used 0.1, meaning it stops if new experiments can't improve accuracy and rather go below 10% of the best value).

As an outcome, we have two models which performed very well (see the table below), with exact same accuracy of 91.49%. Nevertheless, if I would pick
one of the two models, I would use the second in the list, as its inverse regularization parameter (C = 0.021) is greatly smaller than the first (C = 0.483), making this
a way more generalizable model than the first one.

![Top Runs](assets/top_runs.jpg)

The different metrics provided by AzureML showed how different trials behaved as we trained them. The images below show relationship between parameters and metrics.

![HyperDrive results](assets/hyperdrive_results.jpg)

As mentioned before, in this case if two models have similar accuracy, I would choose to use the one with smaller inverse regularization strenght ("C" parameter) as this would make my model more general and fit for unseen data.

## AutoML
The best performing model generated by AutoML was a Voting Ensemble, achieving an accuracy (same primary metric as our other experiment) close to 91.7%. AutoML automatically tried a compendium
of models and tuning, which was the main reason why the general performance was better than tuning only one model, as done in the
previous experiment. Below you can see which were the top models found by AutoML.

![Top models](assets/top_models.jpg)

The results above where obtained by using the following configurations for AutoML:
```python
automl_config = AutoMLConfig(
    experiment_timeout_minutes=30,
    task='classification',
    primary_metric='accuracy',
    training_data=ds.to_pandas_dataframe(),
    label_column_name='y',
    n_cross_validations=2)
```
Now, why did we chose the above? Let me go a bit deeper into these configurations, and what do they mean.

- First of all, the `experiment_timeout_minutes` assured that AutoML would try different types of models until this time threshold was reached.
Going further on, would have meant more costs at (most likely) little return. Besides, we had a time limit of 2 hours for the provided virtual machine, which gave us a threshold for experimentation.

- Secondly, the task refers to the objective of the jobs. We chose `classification` as our labels are only two types of labels ('yes' and 'no'). Other types of possible 
tasks can be seen in [AutoML configuration documentation](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-configure-auto-train).

- Third, we chose `accuracy` as primary metric. A valid question here would be, why would you choose accuracy if other metrics (for example, AUC weighted accuracy) might have been a better, choice given an imbalanced dataset? The main reason was, that our baseline model was also evaluated and tuned for accuracy, so we need to both experiments
to be comparable. Also, after having a deeper look into other metrics, the Voting Ensemble got incredible results (98% AUC weighted accuracy).

- The fourth and fifth columns refer to the training data, which is passed as a DataFrame and the label column name, so AutoML knows against which column is it going
to be training for.

- Last but not least, we chose to have 2 `n_cross_validations`. What does this mean, and why was this? In ["how to configure training, validation, cross-validation and test data in AutoML"](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-configure-cross-validation-data-splits) we can see that this is an optional parameter for selecting the way we handle validation and testing splits of our data, and there are many ways and combinations to do so. If we wouldn't have included it, AutoML would have selected by default a validation mode depending on the size of our dataset. However, we chose to apply 2-fold cross-validation which means we would take 1/2 of our data for training and 1/2 for validation, expecting our resulting model to be very generalizable.

## Pipeline comparison
The pipelines to train models where totally different, as well as the model architectures. In this regard, in the Logistic Regressor tuning we had only one model and
had to take care of data preprocessing, while on AutoML the resulting model including an ensemble of XGBoost, Stochastic Gradient Descent, LightGBM and a Logistic Regressor,
each with different normalization techniques (standard scaling or maximum absolute scaling), which can be seen in the image below.

![AutoML Pipelines](assets/ensemble_details.jpg)

While the Logistic Regressor demanded slightly more domain knowledge and effort by the user, 
as you would have to have picked the model first and have a sense of how to manipulate its hyperparameters,
AutoML takes a lot of effort out of the Data Scientist, by performing dozens or even hundreds of experiments with minimal customization.
The pipeline chosen by AutoML also tried ensemble of models, resulting in the one we had. Additionally, in AutoML you get
a model explanation that also gave us details on the most important features considered in the model (see below).

![Features](assets/top_features.jpg)

## Future work
There are many areas of improvement for next trials. Next time I will train a model, I would certainly go for AutoML on the first shot and 
afterwards try to play around with the best performing models. While a bit more computationally intensive and costly, 
AutoML could save me some time with experimentation, that I would rather use later for better tuning once I chose one of the models.

On the other hand, I would also play around more with the AutoML configuration class, to see what other kinds of metrics can be optimized
for other use cases, such as the percentage of data used for training and the primary metrics available. Additionally, I'd like to 
improve in the sense of picking and inspecting other runs and saving them locally for later use.

## Proof of cluster clean up
The last line of the [Jupyter Notebook](udacity-project.ipynb) contains the relevant method for cleaning up resources.
```python
compute_target.delete()
```
